{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "## What we'll cover\n",
    "\n",
    "* [IDA vs EDA](#IDA-vs-EDA)\n",
    "* [Tools for EDA](#Tools-for-EDA)\n",
    "* [EDA Methods](#EDA-Methods)\n",
    "* [Data Sources](#Data-Sources)\n",
    "* [Data Acquisition](#Data-Acquisition)\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Introduction\n",
    "\n",
    "So let's set some expectations up front:\n",
    "\n",
    "* you are not expected to be an expert after this course\n",
    "* you are not really expected to remember 80% of what we cover\n",
    "\n",
    "However, the one thing to remember is that when it comes to data analysis, you have options.\n",
    "\n",
    "Data analysis with python is at its core a discussion of how you can use tools in the python programming language to automate parts of your existing job duties:\n",
    "\n",
    "* you are not expected to be a python programmer after this course\n",
    "\n",
    "However, you are expected to be an analyst who also has some vague python skills that are \"in development.\"\n",
    "\n",
    "The intent of this course is to introduce the idea of using python tools for data analysis:\n",
    "\n",
    "* giving you basic knowledge of the python programming language\n",
    "* demonstrating basic tools and techniques common in data analysis with python\n",
    "* provide you with reference materials for additional learning\n",
    "\n",
    "This is by no means comprehensive, so if there is content you want added to the course or changed let me know and we'll put it on the update list.\n",
    "\n",
    "For additional details, see the course syllabus in the README file of this repository.\n",
    "\n",
    "So let's get into it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "## IDA vs EDA\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is by no means a universal approach, but it is reasonable to break \"data analysis\" into phases which ensures your efforts are focused and produce results that help you in your work.\n",
    "\n",
    "For the purpose of this course, I've broken data analysis into two basic phases: initial data analysis and exploratory data analysis.\n",
    "\n",
    "Note: your unique form of data analysis may diverge from this model based on your education, specialization, tooling, and the type(s) of data being analyzed. The intent of this model is to make you think about your particular forms of data analysis and how python tools may augment and/or accelerate those forms in the future.\n",
    "\n",
    "Let's define and think about each of these phases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Initial Data Analysis\n",
    "\n",
    "Initial, or prepatory, data analysis can be thought of as the \"first impressions\" phase of the data analysis process. In this phase, we are concerned about what data we have, if it's the data we need, and what tools we should use to answer the questions we want to answer. In many ways, this is a qualitative step to ensure that the data in question, in part or in whole, represents the \"things\" we want to analyze and assess. So we may ask questions like:\n",
    "\n",
    "* What does the data look like?\n",
    "  * What format is it in?\n",
    "  * How much data do I have?\n",
    "* What does this data represent?\n",
    "* Is there anything wrong with the data?\n",
    "  * What is missing from the data that I need for my analysis?\n",
    "  * What needs to be improved in the data?\n",
    "* How was the data collected?\n",
    "  * Who created the data?\n",
    "  * Is that the same as the people who gave it to me?\n",
    "* How is the data being delivered or obtained?\n",
    "  * How long will this take?\n",
    "  * Where do I need to store it?\n",
    "\n",
    "These are a few questions which will help profile your data within the context of your analytic project, guiding you in the selction of tools and techniques necessary and/or desirable for the next phase of analysis. Simply put, you want to know if you have the right data for the job.\n",
    "\n",
    "For example, you may have a simple table with a few hundred records (e.g., rows) and not many columns (e.g., fields). Depending on what you need to do with this data, Excel may be the simplest, most effective tool to accomplish your analysis. However, if you have several tables with thousands of records that exist on a web page, an automated solution with python may be a better, long-term solution.\n",
    "\n",
    "Working through your initial data analysis will also help you better approach other analysts, scientists, and engineers in the case you need assitance with coding or additional data requirements.\n",
    "\n",
    "This step may include basic descriptive statistics, but I often caveat this by advising that you only do as much as necessary to sufficiently assess the completeness of your data for more advanced normalization and transformations. Those are often best left for the next phase once you have ensured the data collection encompasses your analytic problem (e.g., you don't want to work on data you may not need).\n",
    "\n",
    "Ultimately, this it where we get the data ready for the more involved and complex phase of exploratory analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "Exploratory data analysis is what most people think of when they hear \"data analysis\" as it involves the manipulation and modeling of data to address an initial set of questions through focused application of analytic tools and techniques. This is often mislabeled or misrepresented as \"data science\" which in addition to data analysis involves much more hermeneutic research, thesis formation, advanced data modeling, and peer review practices (e.g., the data analysis is situated within a broader context of disciplinary literature and is geared towards hypothesis testing and continued research). In both cases, accurate profiling of data and data exploration are critical to producing rigorous and verifiable results.\n",
    "\n",
    "For the purpose of this course, exploratory data analysis is the process of efficiently providing verifiable results for analytic assessments based on the available data. You may want to ask questions such as:\n",
    "\n",
    "* Which variables (i.e., columns or fields) are important for my analytic question?\n",
    "* Which models best suit the data I have for my analysis?\n",
    "  * Does this data model improve my understanding of the analytic problem?\n",
    "  * How do variables need to be normalized or transformed?\n",
    "  * Do I need to sample my data? If so, how?\n",
    "* Does the output sufficiently answer the analytic question?\n",
    "  * Does the data require more advanced analysis/modeling?  \n",
    "* Does the output change the analytic question?\n",
    "  * Does the output change the context of the analytic question?\n",
    "  * Do you need additional data sets?\n",
    "  * Do you need more of the same data?\n",
    "* Which visualization methods best represent the data model results?\n",
    "  * Is there a risk of misrepresentation?\n",
    "  * Does this cover the necessary scope of analysis?\n",
    "\n",
    "This is where we see if the data is suited to address a part or the whole of an analytic question. It is assumed that you have an understanding of statistics prior to initiating this phase (or at least how to interpret statistical results from your data models).\n",
    "\n",
    "Throughout this process, it is highly encouraged to work with other team members to ensure your data and data model selections make sense and that results pass a sanity check.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ** Add IDA and EDA examples - images and diagrams**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "## Tools for EDA\n",
    "\n",
    "<hr>\n",
    "\n",
    "This course is focused on the use of data analysis tools built with the python programming language. In other words, we are learning to script our own applications in order to \"do something\" with data. However, there are many other tools that can use python as a feature or can be externally automated using python. In later sections, we will cover these types of python approaches in greater detail. What follows is a brief overview of the varying levels of python usage in the context of data analysis tooling and levels of implementation effort.\n",
    "\n",
    "### Software\n",
    "\n",
    "For most people, data analysis tooling is introduced in a form that doesn't require them to build their own application. Often this tooling involves a lot of \"point and click\" manipulation of the data with minimal scripting. At this time, these seemingly simplified tools contain a miriade of powerful analytic features that require you to have basic knowledge for interpretation of the results. These solutions are often constructed with a graphical user interface (or a GUI) in order to create an optimized user experience for the features that software solution was created to fulfill. There are two main categories of software you will encounter most: commercial and open-source software.\n",
    "\n",
    "  * Commercial software, often referred to as Commercial off-the-shelf or COTS, are purpose-built solutions for a distinct set of tasks. Tableau is a peice of commercial software through which tabular data can be input in order to produce analytic visualizations of your data. COTS software often comes with a GUI through which the majority of tasks can be executed and managed, but may also come with an application programmable interface (or API) for use with a programming language or a domain-specific language (or DSL) that was developed exclusively to work with the commericial software. For example, FNA has a DSL through which data is turned into web dashboards. This type of software is generally licensed with some type of support and warranty which helps in case there are issues with setup or use.\n",
    "  * Open-source software, often referred to as free open-source software or FOSS, are also purpose-built solutions for a set of tasks, but come with much more risk in terms of potential breakage, lack of support, and overall difficulty of implementation and management. Often the source code for this software is openly available for review, but this is only useful if you can understand the source code. That being said, the most popular and regularly-used programming languages are open-source and have large communities of users and maintainers providing ample amounts of free training materials and support through websites like Stack Overflow or blogs. PostgreSQL is an open-source database that is widely used by companies and government around the world.\n",
    "    * Open-source is free as in free to use, study, modify, and share the software with others. This is different than \"freeware\" or \"shareware\" in which the application can be used (such as Tableau Public), but source-code is not open and an end-user license agreement (or EULA) restricts what you can and cannot do with your copy of the software.\n",
    "\n",
    "Even after you learn to program, you may find it much more expedient to use a pre-built software solution for your analysis. Programs like Excel or QGIS provide solutions to a vast array of common data analysis questions without having to perform any additional scripting. These solutions also have Python functionality built into them and can also be manipulated externally by python through an API.\n",
    "\n",
    "However, if these just won't suffice, there are several programming solutions available to you for data analysis.\n",
    "\n",
    "### Programming Languages\n",
    "\n",
    "Have you ever seen a movie in which the hoodie-wearing protagonist randomly types on a keyboard, \"code\" from some obscure programming language appears on the computer screen, and then half of New York City descends into chaos? Unfortunately, programming languages are not nearly that exciting... at first. Simply put, programming languages are human-reable systems of logic in which machine-interpretable instructions are created with the intent to make a machine \"do something\". In our case, we want our machines to take data in a digital format and turn it into more useful data (usually as fast as possible because \"reasons\"). There are several different types of programming languages, such as assembly or compiled languages. Python is often referred to as a scripting language as it is often used to orchestrate tasks to and from external programs, but it is in fact a just-in-time compiled language. Remembering that is not important. What is important to know is that because Python has this feature, it can act as a high-level general purpose language which works with many other programming languages and as a result, many other programs. This is very good as our goal is to analyze data from numerous sources in many different formats using as few tools as possible. Other languages such as the R programming language provide similar functionality for exploratory data analysis.\n",
    "\n",
    "### Visualization\n",
    "\n",
    "At the end of exploratory data analysis, you will want to share your results with others. Written research reports with detailed tables expounding upon the complexity of the analysis and nuanced methodologial approach taken would be the primary vehicle for sharing results, but in a world obsessed with PowerPoint and generalization, carefully constructed data visualizations will be the \"go-to\" solution for sharing contextualized exploratory data analysis outputs. Plots, charts, graphs, and diagrams are a few of the many types of visualization available, each dependent upon the type of data being represented (this process will be covered in greater detail later in this course). There are a few general principles that should be observered for visualizations and throughout the data analysis process more broadly. \n",
    "\n",
    "  * A title that clearly delimits the visualization's content\n",
    "  * Clearly labeled elements to further clarify the visualization's rationale\n",
    "  * Additional descriptive text for sources and necessary caveats\n",
    "  * An understanding of who will be the primary viewers of this visualization\n",
    "  * A standard to ethically accomodate the primary viewers\n",
    "  * An assessment of risk via misinterpretation (both foreseen and unanticipated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "## EDA Methods\n",
    "\n",
    "<hr>\n",
    "\n",
    "When performing EDA, method selection is at the core of the analytic process. However, we need to diambiguate the term \"method\" before getting further along in this course. Methodology is the overarching research approach which outlines how the data is to be analyzed and why this approach is valid in the scope of a larger research question. A method is a specific form of analysis through which the data will be processed as dictated by the methodology, the output of which is used to prove or disprove a premise asserted in the research question.\n",
    "\n",
    "There is another \"method\" we will encounter in this course. A python method is a function associated with a python class or object. This is a term as applied within the python nomenclature not associated with the methodology context above.\n",
    "\n",
    "Now that we have those terms defined, we can now briefly outline two major domains of methods you will encounter in the course of EDA: quantitative and qualitative.\n",
    "\n",
    "### Quantitative Methods\n",
    "\n",
    "Quantitative methods are methods that are focused on the analysis of measurement, often leveraging numerical comparisons or statistical inferences in the course of analysis. In other words, these methods are concerned with what numbers to crunch, how to crunch said numbers, and ultimately how interpret the crunched outputs.\n",
    "\n",
    "  * Descriptive statistics are perhaps the simplist forms of quantitative analysis often associated with profiling, or describing, the data for more focused analysis. This method is also used to a great extent as a precursor for qualitative methods such as simple counts of elements.\n",
    "  * Inferential statistics process the data with the goal of drawing conclusions associated with a research question.\n",
    "\n",
    "Metholodologies leverage methods from both of these areas for predictive, prescriptive, and/or categorical analysis in varying levels of complexity. Rudimentary EDA methods are often focused on developing a deeper understanding of the data that ultimately guides application of the more advance methodologies.\n",
    "\n",
    "### Qualitative Methods\n",
    "\n",
    "So what happens when you can't number your way out of an analytic challenge? Qualitative methods. For analysts and researchers, the vast majority of work is done qualitatively. A qualitative method is a method that describes qualities or characteristics of the data. These methods are usually applied to data sources such as large corpora of text, interviews, observational ethnographies, and visual artifacts to name a few. These methods often involve the process of manually coding the data using an explicit heuristic model. There are several machine learning methods that act as quantitative equivalents to these methods, but often require the application of metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "<hr>\n",
    "\n",
    "Before we get into the setup and learning of Python for EDA, a few quick points on the data itself. Initially, there are a few important categories of data to be aware of as their difference can affect the way in which you analyze the data:\n",
    "\n",
    "  * Primary\n",
    "  * Secondary\n",
    "  * Metadata\n",
    "\n",
    "Primary data is data that you've created yourself. This means you've either directly measured and/or observed this data and the methods through which it was collected. The implication of this is that your knowledge and description of the data is a first-hand account of what the data was intended to represent and that analysis of this data gives you a much better idea of what to expect as a result of analysis.\n",
    "\n",
    "Secondary data is data that you've obtained pretty much anywhere else. This means you've downloaded it, scraped it from a website, handed to you by a data courier: any method of collection other than first-hand. The implication of this is you are dependent on other's description of the data, how it was collected, and what they say it represents. This could make modeling more precarious as your confidence in the analytic results are only as good as you grasp of the data source quality.\n",
    "\n",
    "In both case, we are both populating and reading out understanding and descriptions of the data into the metadata. This is data that is embedded or seperately encoded to give the user of a data source as many detail as needed to allow accurate and precise use of a data set by another analyst. This should include the indentity of the original data creator, date of creation, methods of collection, brief summary of how to use the data, data dictionary for the fields or variables, and others. The metadata will depend on the type of data or the collection method, but as we'll see these are generally provided in a smaller set of common file formats.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Data Acquisition\n",
    "\n",
    "<hr>\n",
    "\n",
    "There is no point in talking about EDA if you don't have any data. However, there is an argument to be made as to what \"having data\" is in the context of analysis. In other words, even if you have data and can analyze it to a valid result, it may be for not if it fails to support a project requirement. This is why careful definition of scope and application for data acquisition is a critical step in the EDA process. IDA is the predessor of EDA for this very reason: to prevent \"garbage in garbage out\" analysis.\n",
    "\n",
    "That being said, you will want to work closely with the data provider and/or customer throughout a project to ensure both the data and metadata used for analysis are:\n",
    "\n",
    "* Scoped within a specific analytic question and the associated methodology\n",
    "* Applied correctly within a method that supports methodolodical assertions\n",
    "* Ensured to be of the highest quality possible by all pertainent parties\n",
    "\n",
    "Once you have the data you need, you should make great efforts to work with data scientist and data engineers to get this data into the data warehouse for processing and cataloging. This includes review and revision of the metadata with inputs from anyone provider, customer, and/or analyst.\n",
    "\n",
    "The final points on data acquisition: Be Nice and Do No Harm.\n",
    "\n",
    "We will cover a lot of different sources and resources, including sometimes very complicated coding solutions to obtain messy data from download sites, APIs, web scraping... but you can often get the data you need and more if you just reach out first and contact the people who maintain the data itself. It is very possible that if you use Python incorrectly to get at data (especially online) you could appear to a website or data provider as a bad actor. You really don't want to go about DDoSing a website and then have to explain to them you just wanted a 10Mb CSV file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Okay! Let's get setup and rolling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "## Resources\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
